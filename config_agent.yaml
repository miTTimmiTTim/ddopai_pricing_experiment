# Run_specific parameters

### Agent functions:

SAC:
  learning_rate_actor: 0.0015        # Increased for faster adaptation in shorter episodes
  learning_rate_critic: 0.005          # Increased for quicker critic updates
  initial_replay_size: 64
  max_replay_size: 600               # Reduced to maintain relevance with current MDP data
  batch_size: 32
  hidden_layers: [64, 32]
  activation: "relu"
  warmup_transitions: 10             # Lowered to initiate training sooner given fewer steps per episode
  lr_alpha: 0.0001                   # Increased for more responsive temperature adjustments
  tau: 0.001                       # Increased to let the target network adapt more quickly in dynamic environments
  log_std_min: -20
  log_std_max: 2
  target_entropy: -2.0               # Adjusted to encourage adequate exploration over shorter horizons
  use_log_alpha_loss: True
  optimizer: "Adam"
  loss: "MSE"
  device: "cpu"


Clairvoyant:
  link: linear
Greedy:
  ex_prices: [[2], [5]]
  link: linear

ILQX:
  ex_prices: [[2], [5]]
  link: linear


TS:
  lam: 1
  reg: -1
  ex_prices: [[2], [5]]
  link: linear

MTS:
  ex_prices: [[2], [5]]
  link: linear
  sigma: 0.5
  lambda_e: 10
  exploration: 1e-2
  c_0: 10
  N: 50

